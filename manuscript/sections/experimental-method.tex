% Experimental method

The following section outlines model architectures and estimation methodology employed to extract dependency between words and phrases in textual comments. To explicate further, consider the following example comment from the analysis dataset: ``Muslims hate gays and want them dead''. When viewed in isolation, individual words such as: ``Muslims'', and ``gays'' are not particularly indicative of toxic motivations --- these words may appear in a variety of perfectly healthy discussions. Toxic motives online become discernible when words appear in particular combinations, such as ``hate gays'', or ``want them dead''. In such examples, it is clear that effective classification of intent requires an understanding of the encoding of syntactic patterns based on relative positions of critical words. Indeed, these are the patterns that word-embedding methodologies aim to capture.

Leveraging three independent word-embedding techniques introduced in Section 3, I estimate the classification performance of four popular machine-learning frameworks: (1) logistic regression; (2) shallow neural networks; (3) deep neural networks; and (4) convolutional neural networks. To evaluate the models, I employ a series of standard metrics used in classification tasks: accuracy, precision, recall, and F1 score. I assess model performance using the total dataset, and then separately across subsets of comments targeting particular identity subclasses. In total, my analysis estimates fourteen combinations of model structure and word-embedding, summarised in Table \ref{table:model-menu}. All models were trained for a total length of ten epochs.

\subsection*{4.1. Logistic regression model}
I implement a binary logistic regression model to predict toxicity. To account for proportionally low prevalence of `toxic' labelled comments in the input dataset, penalties for toxicity class weights are set to be inversely proportional to the prevalence of classes in the input dataset. Subsequently, the estimation process imposes larger penalties for inaccurate classifications of `toxic' labels, as compared to `non-toxic' labels. L2 regularisation is applied to weights during the estimation process.

\subsection*{4.2. Shallow neural network (NN) model}
Given substantial non-linear processing has already occurred in pre-trained sentence-BERT and GloVe word-embeddings, an extensive number of hidden layers may not be required to sufficiently capture remaining non-linearity in the input-output relationship. Accordingly, I implement a shallow neural network (NN) consisting of a pre-trained word-embedding layer; one fully-connected hidden layer with one-hundred and twenty-eight nodes activated by the Rectified Linear-unit (ReLU) activation function; and a single-node output layer activated by the sigmoid function.

\subsection*{4.3. Deep neural network (DNN) model}
Deep neural networks are well-studied to exhibit exceptional performance in sentiment-analysis tasks. I implement a DNN consisting of a pre-trained word-embedding layer; two fully-connected hidden layers with one-hundred and twenty-eight nodes each; and a single-node output layer with a sigmoid activation function. As with the NN, hidden nodes are configured with Rectified Linear-unit (ReLU) activation functions.

\subsection*{4.4. Convolutional neural network (CNN) model}
I estimate a relatively simplistic convolutional neural network (CNN), consisting of a pre-trained word-embedding layer; a one-dimensional convolutional layer with a unit stride, and Glorot Normal kernel initialisation; a max-pooling layer with a unit-stride and pool-size equal to two; a single fully-connected hidden layer with one-hundred and twenty-eight nodes; and a single-node output layer activated by the sigmoid function. All nodes in hidden and convolutional layers are configured with Rectified Linear-unit (ReLU) activation functions.

\begin{table}[h]
	\caption{Model structure and word-embedding combinations \label{table:model-menu}}
    \centering
    \begin{tabular}{lllll}
        \toprule
        Model & Word-embedding methodology \\
        \midrule
		\addlinespace{}
        Logistic regression & \parbox{7.5cm}{TF-IDF embeddings}\\
		\addlinespace{}
		Logistic regression & \parbox{7.5cm}{sentence-BERT embeddings}\\
		\addlinespace{}
		Logistic regression & \parbox{7.5cm}{GloVe embeddings (minimum across 100 dimensions)}\\
		\addlinespace{}
		Logistic regression & \parbox{7.5cm}{GloVe embeddings (maximum across 100 dimensions)}\\
		\addlinespace{}
		Logistic regression & \parbox{7.5cm}{GloVe embeddings (average across 100 dimensions)}\\
		\addlinespace{}
        Shallow neural network (NN) & \parbox{7.5cm}{TF-IDF embeddings}\\
		\addlinespace{}
		Shallow neural network (NN) & \parbox{7.5cm}{sentence-BERT embeddings}\\
		\addlinespace{}
		Shallow neural network (NN) & \parbox{7.5cm}{GloVe embeddings}\\
		\addlinespace{}
		Deep neural network (DNN) & \parbox{7.5cm}{TF-IDF embeddings}\\
		\addlinespace{}
		Deep neural network (DNN) & \parbox{7.5cm}{sentence-BERT embeddings}\\
		\addlinespace{}
		Deep neural network (DNN) & \parbox{7.5cm}{GloVe embeddings}\\
		\addlinespace{}
		Convolutional neural network (CNN) & \parbox{7.5cm}{TF-IDF embeddings}\\
		\addlinespace{}
		Convolutional neural network (CNN) & \parbox{7.5cm}{sentence-BERT embeddings}\\
		\addlinespace{}
		Convolutional neural network (CNN) & \parbox{7.5cm}{GloVe embeddings}\\
        \bottomrule
    \end{tabular}
\end{table}