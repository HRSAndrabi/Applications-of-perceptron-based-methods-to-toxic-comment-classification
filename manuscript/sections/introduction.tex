% Introduction

Ours is the age of cyberspace: now, more than ever before, individuals possess unrestrained freedom to anarchically express their opinions for all to behold. 
Freedom of speech is a desirably quality in systems of social interaction, though it cannot be denied that prevailing online culture champions polarising behaviours that are insular to empathetic considerations.
It is against this contextual background that the majority of modern social interactions transpire --- social interactions that exploit, by and large, the traceable anonymity of online systems and the afforded protection against social accountability. 
Indeed, the rise of the internet has transformed the primary setting of our social interactions: but can we be trusted to handle our newfound freedom in a responsible way?
In the absence of naturally civilised interactions, moderation becomes crucial to maintaining positive and healthy discussions --- yet, the volume of online communication is too vast to be effectively moderated in a manual fashion. This raises an important and imminent question: is it possible to automate supervision of the inexorable flow of online interactions in such a way that limit toxic behaviour?

Given these analytical problems and the impending social importance of developing measures to promote healthy online interactions, the analysis in this note will empirically examine the mitigating capacity of machine-learning techniques to identify and filter out textual instances of toxic behaviour.
To this end, I focus my analysis on the assessment of the joint classification capacity of: (1) a range popular of machine-learning techniques known to be accurate classifiers of toxic behaviour; and, (2) a suite of text-embedding representations for numerical encoding of textual data. 
In particular, I consider the capacity of these model-embedding combinations to minimise incidence of biased classifications: that is, the erroneous tendency for discriminatory classification against sensitive classes such as race, religion, and gender. The resulting experimental analysis intends to address the following research questions:

\begin{itemize}
	\item[] \textbf{RQ1}: How does the choice of model and its architecture influence discriminatory biases in classification?
	\item[] \textbf{RQ2}: How does the choice of word-embedding techniques influence discriminatory biases in classification accuracy?
	\item[] \textbf{RQ3}: Which combination of model-architecture and word-embedding technique is the best-suited to toxic comment classification?
\end{itemize}


The remainder of the paper is structures as follows. Section 2 provides a contextual overview of relevant work in the field of toxic comment classification. Section 3 outlines the dataset and pre-processing methods employed in this analysis. Section 4 describes the general experimental framework, and the architecture of studied classification models. Section 5 presents and discusses the results. Finally, Section 6 concludes the paper.


% In general, incidences of extreme behaviour are enabled by two critical features of online systems: (1) traceable anonymity; and (2) visual anonymity. 
% The former refers to the inability of agents to be held to account for offensive behaviour due to time and resource constraints associated with tracing offending online activities back to the originator in the physical world.
% As a result, we observe in online interactions a greater tendency for employment of abusive and anti-social tactics that go far beyond what might be considered acceptable in face-to-face setting.

% insensitive to the threat of social accountability for offensive behaviour. While anonymity alone is not sufficient to solely induce anti-social or abusive behaviour, it promotes insular tactics or language that go far beyond what might be employed in soci

% the corresponding lack of social accountability encourages insular, and often impolite behaviour, as well as the employment of tactics or language that go far beyond social expectations of reasonable behaviour in face-to-face-interaction.

% The mechanism of social accountability for one's behaviour is critical to moderating one's inclination towards abusive or anti-social behaviour. 

% In face-to-face interactions, we are 

% Online social interactions are exempt from the that come with the social accountability of face-to-face interaction.

% Where abusive or anti-social behaviour would be moderated in circumstances of physical social interaction 


% Online social interactions are relatively exempt from the influence of social accountability present in face-to-face social interaction, which serves to moderate one's inclination to engage in abusive or anti-social behaviour.

% In face-to-face social interactions, the immediate social accountability engendered by physical proximity serves to moderate one's inclination to engage in abusive or anti-social behaviour.

% Face-to-face social interactions are characterised by a particular quality of social accountability


% Online interactions are not governed by the same social pressures of face-to-face interactions.

% In contrast to physical social interactions, online social interactions are not regulated by the threat of immediate social accountability for anti-social and abusive behaviour. 


% In online social interactions, one's tendency towards anti-social or abusive behaviour is not moderated by the threat of immediate social accountability that supervises our face-to-face interactions.


% In contrast to face-to-face interactions, tendencies towards anti-social and abusive behaviour in online social interactions are comparatively unfettered by the threat of immediate social accountability. While anonymity alone is not sufficient to solely induce anti-social or abusive behaviour, the corresponding lack of social accountability encourages insular, and often impolite behaviour, as well as the employment of tactics or language that go far beyond social expectations of reasonable behaviour in face-to-face-interaction.


% In contrast to face-to-face interaction, interactions occurring online are typically anonymous, and insensitive to the threat of social accountability for offensive behaviour. While anonymity alone is not sufficient to solely induce anti-social or abusive behaviour, the corresponding lack of social accountability encourages insular, and often impolite behaviour, as well as the employment of tactics or language that go far beyond social expectations of reasonable behaviour in face-to-face-interaction.


% The lack of accountability in online social interactions encourages insular behaviour, and the employment of tactics that go far beyond that which might be employed in face-to-face interaction.