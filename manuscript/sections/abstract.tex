% Abstract
Interactions occurring online are typically insensitive to the threat of social accountability for toxic behaviour, leading to the employment of abusive, and anti-social tactics that go far beyond what might be considered acceptable in face-to-face settings. While applications of machine learning techniques to automatically detect and classify toxic behaviour are well-studied, it has proven difficult to occlude biases in the training datasets from flowing onwards to the classifier, and thereafter contributing to discriminatory classifications against sensitive classes such as race, religion, and gender. In this note, I seek to illuminate the capacity for word-embedding techniques to suppress undue learning of sensitive biases in training datasets. I consider this objective in the context of two popular deep-learning frameworks for toxic comment classification: long short-term memory (LSTM) networks, and convolutional neural networks (CNNs). I demonstrate that [...].

% While anonymity of online social interactions alone is not sufficient to solely induce anti-social or abusive behaviour, it promotes insular attitudes and the employment of tactics that go far beyond what might be employed in face-to-face settings.