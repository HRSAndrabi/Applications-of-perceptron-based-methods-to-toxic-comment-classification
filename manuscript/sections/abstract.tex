% Abstract
Interactions occurring online are typically insensitive to the threat of social accountability for toxic behaviour, leading to frequent employment of abusive and anti-social tactics that go far beyond what might be considered acceptable in face-to-face settings. 
Automated machine learning techniques can detect and classify instances of toxic behaviour when they occur, however it has proven difficult to occlude biases in training datasets from flowing onwards to the classifier, resulting in discriminatory classifications against sensitive classes such as race, religion, and gender. 
In this paper, I seek to illuminate the capacity for combinations of model architecture and word-embedding techniques to suppress undue influence of such discriminatory classification.
I consider this objective in the context of two popular perceptron-based approaches to toxic comment classification, and identify convolutional neural networks augmented with GloVe embeddings as the superior classifier.

% While anonymity of online social interactions alone is not sufficient to solely induce anti-social or abusive behaviour, it promotes insular attitudes and the employment of tactics that go far beyond what might be employed in face-to-face settings.